{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 过拟合\n",
    "神经网络有着非常惊艳的拟合能力，上面我们已经看到了神经网络可以逼近函数和概率分布，实现多种有趣的任务。但是和上面的玩具数据集做出来的结果不同，神经网络在实际问题里非常有可能出现更为严重的过拟合。而除了上面的正则化方法以外，我们还有其他方法控制过拟合。  \n",
    "**Eealy Stop**  \n",
    "一种相当直觉的方法，但是经常能够凑效。我们在训练时用一个验证集辅助训练，一旦训练出现了让测试集loss下降的趋势就立刻停止训练。  \n",
    "**DropOut**  \n",
    "从模型融合中得到启发，做过Kaggle的人应该有经验，在模型和算法已经完善的末期，我们进一步提升正确率的方法就是在模型中引入随机因素，制造很多的模型形成委员会，用这个混合的模型进行任务决策。而神经网络自己本就是一个混合模型，如果我们使用dropout的技巧，即每次训练时，随机临时删除一些 神经元，它们将不在这次运算中发挥作用，同样也不会被更新。这样，一个神经网络就相当于多个小型神经网络的混合模型，这个方法在大型任务重常常很实用。  \n",
    "**更好的数据集**  \n",
    "更多更好的数据是防止过拟合的最好方法，即使没有更多的数据，我们也可以用一些技巧创造数据。这个做法在图像和语音领域尤其适用，我们可以随机对图片进行旋转，平移，加噪声，来获得更多更稳定的数据集。  \n",
    "**reinforcement**\n",
    "强化学习是一种特殊的方法，它基于模型的对抗进行。在对弈和决策问题里，我们可以通过模型对抗的方式让模型自行学习，从而获得超越数据集的成效。最好的例子就是击败人类的alphago。\n",
    "### Dropout\n",
    "网络过于习惯训练集的数据分布, 尤其是如果训练集规模还不够大时, 网络会出现严重的过拟合, 使得在训练集上的准确率非常高, 但是测试集上的准确率很低. 而且这个过程还不可逆, 一旦网络过拟合那网络就废了, 我们的努力全部木大.  \n",
    "神经网络中的一种有效的对抗过拟合的方法是dropout, 它基于模型融合的思想, 让当前隐层的一些神经元暂时死掉, 它的输出将是0 ,从而不对本次前向和反向传播产生任何影响. 这样做的好处是, 如果这样的模型仍然能被训练到收敛, 那么我们可以认为我们从隐层中抽取出一定比例的神经元也仍然是收敛的. 然而这样随机采样得到的很多小一些的子网络与子网络之间是有些微的差异的, 这些差异经过模型融合, 能让总网络变成一个不具有倾向性的模型(委员会思想). 从而模型的偏差可能会上升一些, 但是方差一般会降低更多.  \n",
    "![dropout.png](dropout.png)\n",
    "它的前向和反向传播非常简单, 我们只需要给定一个概率, 然后它就会生成一个mask把一定比例的x置零再输出. 反向传播时, 因为mask处的x被置零了, 就相当于点乘了一个0常数, 那么它的反向传播也是乘一个0, mask把对应位置的dz置零再反向传播.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch-Normalization\n",
    "接下来我们要讲一下, 能让训练加速的一种技巧, 叫做batch norm. 相对于卷积和循环的网络, 这种技术的出现是2015年, 是很年轻的技术. 这种方法针对训练中(尤其是在深度网络里)遇到的困难提出一些解决方案, 之前我们的代码实践虽然是深度学习, 但并不算是非常deep的网络, 真正很深的大型网络在训练时会面临Internal Covariate Shift的问题. 简单来说, 我们的反向传播计算梯度是逐层进行的, 当我们更新了第l层的参数, 第l层的输出也会改变, 从概率的角度解释就是分布发生了改变. 这样l+1层就需要去适应新的输入分布, 这个过程需要时间. 可怕的是, 当网络很深时, 这个适应过程就会非常复杂且困难, 为了加速深度网络的训练, 有一种想法是, 我们人为地改变数据分布, 每一隐层无论何时, 分布都是相似的. 一种方法就是规范化方法, 我们把数据分布减去分布的均值再除以标准差, 这样分布就会被规约到和标准正态分布类似. 注意这里的规范化是对每一维度独立进行的, 而不是计算样本集的协方差矩阵, 那样会浪费大量算力.  \n",
    "$$ \\mu_j =  \\frac{1}{m}\\sum_{i=1}^m Z_{ji} $$\n",
    "$$ \\sigma_j^2 =  \\frac{1}{m}\\sum_{i=1}^m (Z_{ji}-\\mu_j)^2 $$\n",
    "$$ \\hat Z_{ji} = \\frac{Z_{ji}-\\mu_j}{\\sigma_j} $$\n",
    "这个做法当然是可以的, 我们在数据科学中使用的一种数据预处理方式就是这种Normalization. 在神经网络中当然也可以用, 而且很适合, 因为大多数我们用到的激活函数都是在输入x=0时斜率最大, 从而梯度最大, 学习速度最快. 如果我们把这种方法应用在隐层间, 就能解决上面Internal Covariate Shift的问题.  \n",
    "但是如果真的那么做, 会出现新的问题. 我们知道神经网络的一大特点就是越深层, 特征就越抽象. 最明显的例子是自编码器, 如果我们用自编码器处理MNIST数据, 并把隐层的特征在2D平面做可视化, 会看见不同类别的数据会被编码到独立的区域. 这就是深度的网络对数据处理后的结果. 但是这时我们用一个这样的norm处理它, 很大可能会把网络已经提取到的不同的特征再次混合在一起, 网络的表达能力遭到伤害. 那么要怎样既能保住底层网络学习到的参数信息, 又能修正数据分布, 方便网络训练呢?  \n",
    "BN的最大贡献就是这部分, 我们再次加入两个参数, 把数据分布再做一个一维线性变换, 把分布拉离0, 并修正方差. 也就是\n",
    "$$ y_{ji} = \\gamma_j\\hat Z_{ji} + \\beta_j $$\n",
    "相当于和上面的规范化相对的一种方法, 如果mu和beta相等, gamma和sigma相等, 这就是一个没有进行变换的变换. 通过上面的步骤，我们就在一定程度上保证了输入数据的表达能力.  有时我们会为了处理分母, 用一个eps来辅助运算, 总的运算方法如下.\n",
    "![batchnorm.png](batchnorm.png)\n",
    "### BN的BP\n",
    "这个过程的反向传播是怎么做的呢? 其实再怎么复杂也无非是代数求导, 只要有耐心推就怎么都推得出来. \n",
    "$$ \\frac{\\partial L}{\\partial \\beta_j} = SUMROW \\frac{\\partial L}{\\partial y_{ij}} $$\n",
    "$$ \\frac{\\partial L}{\\partial \\gamma_j} = SUMROW \\frac{\\partial L}{\\partial y_{ij}}*Z_{ij} $$\n",
    "$$ \\frac{\\partial L}{\\partial Z_{ij}} = \\gamma_j \\frac{\\partial L}{\\partial y_{ij}}\\frac{\\partial\\hat Z_{ij}}{\\partial Z_{ij}} $$\n",
    "下面的部分才比较复杂, 均值的偏导比较容易, 但是方差不但有分数形式还与均值有关, 我们分三步来看向量形式的求导\n",
    "![bn_bp0.PNG](bn_bp0.PNG)\n",
    "这里抄一张图, batchnorm的计算图展开, 可视化的计算可能会更为清晰\n",
    "![bn_bp.png](bn_bp.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T05:34:23.548567Z",
     "start_time": "2020-08-11T05:34:22.742924Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "path = os.getcwd()\n",
    "os.chdir('..')\n",
    "from deepnotes import *\n",
    "os.chdir(path)\n",
    "# 验算梯度用\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T05:34:24.582316Z",
     "start_time": "2020-08-11T05:34:23.619255Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:\n",
      " tensor([[ 0.0965, -0.0521],\n",
      "        [ 1.1832,  0.1047],\n",
      "        [-1.8249, -0.1628],\n",
      "        [ 0.0361,  1.6289],\n",
      "        [ 0.5091, -1.5187]], grad_fn=<NativeBatchNormBackward>)\n",
      "weight grad:\n",
      " tensor([10.3113,  6.8323])\n",
      "bias grad:\n",
      " tensor([ 0.9334, -5.1370])\n",
      "x grad:\n",
      " tensor([[ 4.2760,  4.0436],\n",
      "        [-0.1107,  0.3687],\n",
      "        [-0.1849, -1.5179],\n",
      "        [-2.9751, -1.4204],\n",
      "        [-1.0053, -1.4741]])\n"
     ]
    }
   ],
   "source": [
    "bn_torch = torch.nn.BatchNorm1d(2)\n",
    "bn_torch.eps = 0\n",
    "bn_torch.momentum = 0\n",
    "bn_torch.train()\n",
    "\n",
    "x = torch.randn(5,2)\n",
    "target = torch.randn(5,2)\n",
    "x.requires_grad = True\n",
    "y = bn_torch(x)\n",
    "\n",
    "loss = F.mse_loss(y,target,reduction='sum')\n",
    "bn_torch.zero_grad()\n",
    "if x.grad:\n",
    "    x.grad.data.zero_()\n",
    "loss.backward()\n",
    "\n",
    "print(\"output:\\n\",y)\n",
    "print(\"weight grad:\\n\",bn_torch.weight.grad)\n",
    "print(\"bias grad:\\n\",bn_torch.bias.grad)\n",
    "print(\"x grad:\\n\",x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T05:34:25.244077Z",
     "start_time": "2020-08-11T05:34:25.226529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:\n",
      " [[ 0.09650204 -0.05210881]\n",
      " [ 1.1832237   0.10468968]\n",
      " [-1.8248808  -0.16284497]\n",
      " [ 0.03606102  1.6289374 ]\n",
      " [ 0.5090945  -1.5186734 ]]\n",
      "weight grad:\n",
      " [10.31134701  6.83230639]\n",
      "bias grad:\n",
      " [ 0.93340707 -5.13704538]\n",
      "x grad:\n",
      " [[ 4.2759824   4.043637  ]\n",
      " [-0.11070609  0.36869228]\n",
      " [-0.18490124 -1.5178801 ]\n",
      " [-2.975091   -1.4203757 ]\n",
      " [-1.0052843  -1.4740726 ]]\n"
     ]
    }
   ],
   "source": [
    "bn = BatchNorm(2)\n",
    "bn.weight = bn_torch.weight.data.numpy()\n",
    "bn.bias = bn_torch.bias.data.numpy()\n",
    "y = bn(x.data.numpy())\n",
    "dx = bn.backward((y-target.data.numpy())*2)\n",
    "\n",
    "print(\"output:\\n\",y)\n",
    "print(\"weight grad:\\n\",bn._dw)\n",
    "print(\"bias grad:\\n\",bn._db)\n",
    "print(\"x grad:\\n\",dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 工程细节\n",
    "在实际使用batch norm时, 深度学习框架通常会把模块定义成两个模式, 一个是训练模式一个是测试模式. 在训练时, 我们的BN层会不断吃数据, 并计算出一个对整个训练集而言的均值和方差, 在测试时, 我们会用这个均值和方差进行计算. 从而让训练时的局部性不会对测试产生影响. 具体到实现上就是train模式和eval模式.  \n",
    "再者就是上面提到的epsilon避免方差过小. 更新batch norm的方法是梯度下降, 我们计算出w和b的梯度后会用梯度优化方法优化参数, 这时pytorch还设置了动量, 避免w和b产生过大的更新. 从实践经验上来看, 这一点的帮助还是相当大的.  \n",
    "这里为了不大改框架，就只实现一个训练模式。事实上，训练模式完全可以用在测试模式里，只不过效果可能略逊一筹。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T05:34:34.889378Z",
     "start_time": "2020-08-11T05:34:30.750739Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# load mnist dataset\n",
    "(x_train_origin,t_train_origin),(x_test_origin,t_test_origin) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train,X_test = x_train_origin/255.,x_test_origin/255.\n",
    "batch_size = 100\n",
    "\n",
    "train_loader = DataLoader(X_train,t_train_origin,batch_size)\n",
    "test_loader = DataLoader(X_test,t_test_origin,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T05:37:51.938886Z",
     "start_time": "2020-08-11T05:34:38.045561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.5630\n",
      "Epoch [1/5], Step [200/600], Loss: 0.2306\n",
      "Epoch [1/5], Step [300/600], Loss: 0.1722\n",
      "Epoch [1/5], Step [400/600], Loss: 0.1585\n",
      "Epoch [1/5], Step [500/600], Loss: 0.1521\n",
      "Epoch [1/5], Step [600/600], Loss: 0.1143\n",
      "Epoch [2/5], Step [100/600], Loss: 0.0966\n",
      "Epoch [2/5], Step [200/600], Loss: 0.0759\n",
      "Epoch [2/5], Step [300/600], Loss: 0.0692\n",
      "Epoch [2/5], Step [400/600], Loss: 0.0630\n",
      "Epoch [2/5], Step [500/600], Loss: 0.0641\n",
      "Epoch [2/5], Step [600/600], Loss: 0.0501\n",
      "Epoch [3/5], Step [100/600], Loss: 0.0460\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0348\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0291\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0328\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0299\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0239\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0247\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0184\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0198\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0217\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0260\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0279\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0235\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0225\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0242\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0234\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0227\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0171\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Train loss(Cross Entropy)')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22d962b33c8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "total_step = len(train_loader)\n",
    "\n",
    "model = Sequential(\n",
    "    Linear(28*28,512),\n",
    "    BatchNorm(512),\n",
    "    ReLU(),\n",
    "    Linear(512,256),\n",
    "    BatchNorm(256),\n",
    "    ReLU(),\n",
    "    Linear(256,128),\n",
    "    BatchNorm(128),\n",
    "    ReLU(),\n",
    "    Linear(128,64),\n",
    "    BatchNorm(64),\n",
    "    ReLU(),\n",
    "    Linear(64,10)\n",
    ")\n",
    "\n",
    "loss_func = CrossEntropyLossWithSoftMax(10)\n",
    "optimizer = Adam(0.001)\n",
    "model.apply_optim(optimizer)\n",
    "\n",
    "loss_list = []\n",
    "log_step = 100 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.\n",
    "    for i in range(total_step):\n",
    "        x,y = train_loader.get_batch()\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        # Forward pass\n",
    "        logits = model(x)\n",
    "        # calculate loss\n",
    "        loss,dlogits = loss_func(logits,y)\n",
    "        # Backward\n",
    "        model.zero_grad()\n",
    "        model.backward(dlogits)\n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if (i+1) % log_step == 0:\n",
    "            running_loss/=log_step\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, running_loss))\n",
    "            loss_list.append(running_loss)\n",
    "            running_loss = 0.\n",
    "\n",
    "plt.plot(loss_list)\n",
    "plt.title('Train loss(Cross Entropy)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T05:37:54.968992Z",
     "start_time": "2020-08-11T05:37:54.361877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 96.98 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for i in range(len(test_loader)):\n",
    "    x,y = test_loader.get_batch()\n",
    "    x = x.reshape(x.shape[0],-1)\n",
    "    # Forward pass\n",
    "    outputs = model(x)\n",
    "    predicted = np.argmax(outputs, axis = 1)\n",
    "    total += y.shape[0]\n",
    "    correct += (predicted == y).sum()\n",
    "    \n",
    "print('Accuracy of the network on the 10000 test images: %.2f %%'%(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
