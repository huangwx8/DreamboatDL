{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "词嵌入在NLP领域几乎是不可或缺。以使用最广泛的英文为例，英文单词总量超过300k，在大规模数据集中，统计到的词通常超过100k。如果使用one hot表示一个词，其表示将是相当稀疏的。这样训练将比较缓慢，内存开销也相当大。  \n",
    "一种有效的方法是使用词嵌入，把词唯一的对应到某个连续n维空间的词向量。\n",
    "![word-embedding.jpg](word-embedding.jpg)\n",
    "其反向传播将使用<SelectBackward>，取下标的操作将对部分参数产生梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T05:41:35.308916Z",
     "start_time": "2020-08-11T05:41:34.783842Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.getcwd()\n",
    "os.chdir('..')\n",
    "from deepnotes import *\n",
    "os.chdir(path)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# 使用Pytorch验算卷积和池化的梯度\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T05:41:36.202381Z",
     "start_time": "2020-08-11T05:41:35.310951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw:\n",
      " tensor([[ 9.6908,  1.6885],\n",
      "        [ 0.1929,  5.4208],\n",
      "        [ 6.4563, -5.2453],\n",
      "        [-2.0448, -1.6821],\n",
      "        [19.7709, -0.1509],\n",
      "        [-8.8800, 13.5255],\n",
      "        [-2.9446, -1.8931],\n",
      "        [-2.3794, -5.8645],\n",
      "        [-6.3412,  2.5517],\n",
      "        [ 7.4903,  6.4962]])\n",
      "dw:\n",
      " tensor([[ 9.6908,  1.6885],\n",
      "        [ 0.1929,  5.4208],\n",
      "        [ 6.4563, -5.2453],\n",
      "        [-2.0448, -1.6821],\n",
      "        [19.7709, -0.1509],\n",
      "        [-8.8800, 13.5255],\n",
      "        [-2.9446, -1.8931],\n",
      "        [-2.3794, -5.8645],\n",
      "        [-6.3412,  2.5517],\n",
      "        [ 7.4903,  6.4962]])\n"
     ]
    }
   ],
   "source": [
    "embed1 = nn.Embedding(10,2)\n",
    "embed2 = Embedding(10,2)\n",
    "embed2.weight = embed1.weight.data.numpy()\n",
    "\n",
    "x_train = torch.randint(0,10,(5,6))\n",
    "x_train_numpy = x_train.data.numpy()\n",
    "y_train = torch.randn(5,6,2)\n",
    "y_train_numpy = y_train.data.numpy()\n",
    "\n",
    "out = embed1(x_train)\n",
    "loss = F.mse_loss(out,y_train,reduction='sum')\n",
    "loss.backward()\n",
    "\n",
    "out = embed2(x_train_numpy)\n",
    "embed2.backward(2*(out-y_train_numpy))\n",
    "\n",
    "print('dw:\\n',embed1.weight.grad)\n",
    "print('dw:\\n',torch.FloatTensor(embed2._dw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "常用的基于语料库训练word embedding的方法是无监督的Word2Vec算法。语料库很容易收集，我们只需要下载真正的人类语言文档即可。  \n",
    "常用的Word2Vec算法有CBOW和Skip-Gram等，CBOW模型是将一个词所在的上下文中的词作为输入，而那个词本身作为输出，也就是说，看到一个上下文，希望大概能猜出这个词和它的意思。而skip-gram模型是将一个词所在的上下文中的词作为输出，而那个词本身作为输入，也就是说，给出一个词，希望预测可能出现的上下文的词。我们认为这两种算法同样有效。\n",
    "![w2v.png](w2v.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T05:41:36.281648Z",
     "start_time": "2020-08-11T05:41:36.205387Z"
    }
   },
   "outputs": [],
   "source": [
    "class CBOW:\n",
    "    def __init__(self, vocab_size, n_dim):\n",
    "        # context size = 2的CBOW模型\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_dim = n_dim\n",
    "        self.embeddings = Embedding(vocab_size, n_dim)\n",
    "        self.predictor = Sequential(\n",
    "            Linear(4 * n_dim, 128),\n",
    "            ReLU(),\n",
    "            Linear(128, vocab_size)\n",
    "        )\n",
    "        self.optim = Adam(0.001)\n",
    "        self.predictor.apply_optim(self.optim)\n",
    "        self.optim.add_module(self.embeddings)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        inputs: int array, shape = (batch_size, 4)\n",
    "        '''\n",
    "        embeds = self.embeddings(inputs)\n",
    "        # (batch_size, 4, n_dim)\n",
    "        embeds = embeds.reshape(-1,4*self.n_dim)\n",
    "        probs = self.predictor(embeds)\n",
    "        return probs\n",
    "    \n",
    "    def backward(self, labels):\n",
    "        '''\n",
    "        dz: 1 d array: shape = (batch_size,)\n",
    "        '''\n",
    "        dx = self.predictor.backward(labels)\n",
    "        self.embeddings.backward(dx)\n",
    "        \n",
    "    def fit(self, ids, num_iters, batch_size):\n",
    "        '''\n",
    "        ids: 1 d array: shape = (text_size,)\n",
    "        '''\n",
    "        loss_func = CrossEntropyLossWithSoftMax(self.vocab_size)\n",
    "        for t in range(num_iters):\n",
    "            indices = []\n",
    "            for _ in range(batch_size):\n",
    "                idx = np.random.randint(0,len(ids)-4)\n",
    "                indices += list(range(idx,idx+5))\n",
    "            x = ids[indices]\n",
    "            x = x.reshape(batch_size,5)\n",
    "            y = x[:,2]\n",
    "            x = x[:,[0,1,3,4]]\n",
    "            logits = self.forward(x)\n",
    "            loss,dlogits = loss_func(logits,y)\n",
    "            if (t+1)%100==0:\n",
    "                print('iters: %d, cross entropy loss: %.4f'%(t+1,loss))\n",
    "            self.embeddings.zero_grad()\n",
    "            self.predictor.zero_grad()\n",
    "            self.backward(dlogits)\n",
    "            self.optim.step()\n",
    "        return self.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T05:41:36.929361Z",
     "start_time": "2020-08-11T05:41:36.925335Z"
    }
   },
   "outputs": [],
   "source": [
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T05:41:37.850848Z",
     "start_time": "2020-08-11T05:41:37.233623Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = Corpus()\n",
    "ids = corpus.get_data('data/train.txt')\n",
    "vocab_size = len(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T06:08:45.046188Z",
     "start_time": "2020-08-11T05:41:38.589535Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iters: 100, cross entropy loss: 6.9052\n",
      "iters: 200, cross entropy loss: 6.6338\n",
      "iters: 300, cross entropy loss: 6.1137\n",
      "iters: 400, cross entropy loss: 5.8990\n",
      "iters: 500, cross entropy loss: 5.6250\n",
      "iters: 600, cross entropy loss: 5.4762\n",
      "iters: 700, cross entropy loss: 5.6873\n",
      "iters: 800, cross entropy loss: 5.3851\n",
      "iters: 900, cross entropy loss: 5.2231\n",
      "iters: 1000, cross entropy loss: 5.2515\n",
      "iters: 1100, cross entropy loss: 5.0302\n",
      "iters: 1200, cross entropy loss: 5.1134\n",
      "iters: 1300, cross entropy loss: 4.9689\n",
      "iters: 1400, cross entropy loss: 4.9152\n",
      "iters: 1500, cross entropy loss: 4.8894\n",
      "iters: 1600, cross entropy loss: 4.6809\n",
      "iters: 1700, cross entropy loss: 4.9487\n",
      "iters: 1800, cross entropy loss: 4.7791\n",
      "iters: 1900, cross entropy loss: 4.5680\n",
      "iters: 2000, cross entropy loss: 4.4625\n"
     ]
    }
   ],
   "source": [
    "model = CBOW(vocab_size, 200)\n",
    "embeddings = model.fit(ids, num_iters = 2000, batch_size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T06:08:55.301054Z",
     "start_time": "2020-08-11T06:08:55.295028Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_word_vec(embeddings,word):\n",
    "    idx = corpus.dictionary.word2idx[word]\n",
    "    vec = embeddings(np.array([idx]))[0]\n",
    "    return vec\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.sum(vec1*vec2)/(np.linalg.norm(vec1)*np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T06:08:55.878168Z",
     "start_time": "2020-08-11T06:08:55.871618Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we - i 0.07368085702862864\n",
      "me - i -0.15907262525966892\n"
     ]
    }
   ],
   "source": [
    "vec1 = get_word_vec(embeddings,'we')\n",
    "vec2 = get_word_vec(embeddings,'i')\n",
    "vec3 = get_word_vec(embeddings,'me')\n",
    "\n",
    "print('we - i',cosine_similarity(vec1, vec2))\n",
    "print('me - i',cosine_similarity(vec2, vec3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T06:08:58.458470Z",
     "start_time": "2020-08-11T06:08:58.451948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog - cat 0.048153883506455455\n",
      "human - cat 0.0679982945061183\n"
     ]
    }
   ],
   "source": [
    "vec1 = get_word_vec(embeddings,'dog')\n",
    "vec2 = get_word_vec(embeddings,'cat')\n",
    "vec3 = get_word_vec(embeddings,'human')\n",
    "\n",
    "print('dog - cat',cosine_similarity(vec1, vec2))\n",
    "print('human - cat',cosine_similarity(vec2, vec3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T06:09:35.989341Z",
     "start_time": "2020-08-11T06:09:35.983351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man - woman 0.0555224650599688\n",
      "girl - woman -0.01660520614757255\n"
     ]
    }
   ],
   "source": [
    "vec1 = get_word_vec(embeddings,'man')\n",
    "vec2 = get_word_vec(embeddings,'woman')\n",
    "vec3 = get_word_vec(embeddings,'girl')\n",
    "\n",
    "print('man - woman',cosine_similarity(vec1, vec2))\n",
    "print('girl - woman',cosine_similarity(vec2, vec3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用word-embedding就可以直接实现文本分类的任务。在embedding的维度上，我们把一个句子的各个词的词向量平均加权，或者用tf-idf加权加在一起，得到表征一个文本信息的句子向量，再在embedding空间中使用k近邻，就能实现简单的文本分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
