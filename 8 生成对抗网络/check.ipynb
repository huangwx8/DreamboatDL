{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T16:42:04.470385Z",
     "start_time": "2020-08-11T16:42:04.456845Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "path = os.getcwd()\n",
    "os.chdir('..')\n",
    "from deepnotes import *\n",
    "os.chdir(path)\n",
    "\n",
    "# 使用Pytorch验算卷积和池化的梯度\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T17:09:50.277611Z",
     "start_time": "2020-08-11T17:09:50.256554Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "z_dim = 64\n",
    "hidden_size = 256\n",
    "image_size = 784\n",
    "num_epochs = 200\n",
    "batch_size = 100\n",
    "\n",
    "D1 = nn.Sequential(\n",
    "    nn.Linear(image_size, hidden_size),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(hidden_size, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "G1 = nn.Sequential(\n",
    "    nn.Linear(z_dim, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, image_size),\n",
    "    nn.Tanh()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T17:09:50.647596Z",
     "start_time": "2020-08-11T17:09:50.588938Z"
    }
   },
   "outputs": [],
   "source": [
    "D2 = Sequential(\n",
    "    Linear(image_size, hidden_size),\n",
    "    LeakyReLU(0.2),\n",
    "    Linear(hidden_size, hidden_size),\n",
    "    LeakyReLU(0.2),\n",
    "    Linear(hidden_size, 1),\n",
    "    Sigmoid()\n",
    ")\n",
    "\n",
    "D2.modules[0].weight -= D2.modules[0].weight\n",
    "D2.modules[0].weight += D1[0].weight.T.data.numpy()\n",
    "D2.modules[0].bias -= D2.modules[0].bias\n",
    "D2.modules[0].bias += D1[0].bias.data.numpy()\n",
    "\n",
    "D2.modules[2].weight -= D2.modules[2].weight\n",
    "D2.modules[2].weight += D1[2].weight.T.data.numpy()\n",
    "D2.modules[2].bias -= D2.modules[2].bias\n",
    "D2.modules[2].bias += D1[2].bias.data.numpy()\n",
    "\n",
    "D2.modules[4].weight -= D2.modules[4].weight\n",
    "D2.modules[4].weight += D1[4].weight.T.data.numpy()\n",
    "D2.modules[4].bias -= D2.modules[4].bias\n",
    "D2.modules[4].bias += D1[4].bias.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T17:09:51.125365Z",
     "start_time": "2020-08-11T17:09:51.058688Z"
    }
   },
   "outputs": [],
   "source": [
    "G2 = Sequential(\n",
    "    Linear(z_dim, hidden_size),\n",
    "    ReLU(),\n",
    "    Linear(hidden_size, hidden_size),\n",
    "    ReLU(),\n",
    "    Linear(hidden_size, image_size),\n",
    "    Tanh()\n",
    ")\n",
    "\n",
    "G2.modules[0].weight -= G2.modules[0].weight\n",
    "G2.modules[0].weight += G1[0].weight.T.data.numpy()\n",
    "G2.modules[0].bias -= G2.modules[0].bias\n",
    "G2.modules[0].bias += G1[0].bias.data.numpy()\n",
    "\n",
    "G2.modules[2].weight -= G2.modules[2].weight\n",
    "G2.modules[2].weight += G1[2].weight.T.data.numpy()\n",
    "G2.modules[2].bias -= G2.modules[2].bias\n",
    "G2.modules[2].bias += G1[2].bias.data.numpy()\n",
    "\n",
    "G2.modules[4].weight -= G2.modules[4].weight\n",
    "G2.modules[4].weight += G1[4].weight.T.data.numpy()\n",
    "G2.modules[4].bias -= G2.modules[4].bias\n",
    "G2.modules[4].bias += G1[4].bias.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T17:09:53.152778Z",
     "start_time": "2020-08-11T17:09:51.529439Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# load mnist dataset\n",
    "(x_train_origin,t_train_origin),(x_test_origin,t_test_origin) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train,X_test = x_train_origin/127.5-1,x_test_origin/127.5-1\n",
    "batch_size = 100\n",
    "\n",
    "train_loader = DataLoader(X_train,t_train_origin,batch_size)\n",
    "test_loader = DataLoader(X_test,t_test_origin,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T17:09:53.298663Z",
     "start_time": "2020-08-11T17:09:53.156786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.8142e-03,  1.8691e-03, -3.9583e-04,  ..., -3.3681e-04,\n",
      "         -3.4774e-03,  9.9738e-04],\n",
      "        [ 3.7846e-03,  1.8627e-03, -3.2522e-04,  ..., -4.7253e-04,\n",
      "         -3.4235e-03,  1.0052e-03],\n",
      "        [ 3.8580e-03,  1.8610e-03, -5.8603e-04,  ..., -2.3947e-04,\n",
      "         -3.5073e-03,  1.0130e-03],\n",
      "        ...,\n",
      "        [ 3.6917e-03,  1.8559e-03, -2.7191e-04,  ..., -5.0256e-04,\n",
      "         -3.4215e-03,  1.0035e-03],\n",
      "        [ 4.3111e-03,  1.8918e-03, -1.1503e-03,  ...,  3.3949e-04,\n",
      "         -3.6607e-03,  1.0602e-03],\n",
      "        [ 4.0914e-03,  1.8685e-03, -8.6573e-04,  ...,  5.8522e-05,\n",
      "         -3.5890e-03,  1.0387e-03]])\n",
      "tensor([[ 2.4733e-05, -1.7784e-05,  3.8453e-05,  ..., -5.9857e-06,\n",
      "          1.1233e-04,  1.5950e-04],\n",
      "        [ 1.2189e-05,  5.1221e-05,  9.4727e-05,  ..., -1.0480e-04,\n",
      "         -5.1127e-06, -1.3556e-04],\n",
      "        [-2.9337e-05, -6.1800e-05, -5.6997e-05,  ..., -9.5516e-05,\n",
      "          1.3723e-05,  6.8221e-05],\n",
      "        ...,\n",
      "        [-2.8875e-05,  1.7340e-05,  7.1711e-05,  ..., -2.1557e-05,\n",
      "          2.5385e-05,  6.0189e-05],\n",
      "        [ 1.7013e-05,  3.4470e-05,  3.8829e-05,  ..., -7.7392e-05,\n",
      "          3.2902e-06,  3.6874e-05],\n",
      "        [-8.1480e-05,  4.3019e-06,  5.3887e-05,  ...,  7.5071e-05,\n",
      "          1.6376e-05,  2.3658e-05]])\n"
     ]
    }
   ],
   "source": [
    "loss_func = BCELoss()\n",
    "\n",
    "images,_ = train_loader.get_batch()\n",
    "images = images.reshape(images.shape[0],-1)\n",
    "real_labels = np.ones((batch_size,1))*0.9\n",
    "fake_labels = np.zeros((batch_size,1))\n",
    "\n",
    "\n",
    "D2.zero_grad()\n",
    "p = D2(images)\n",
    "d_loss,dp = loss_func(p,real_labels)\n",
    "D2.backward(dp)\n",
    "\n",
    "z = np.random.randn(batch_size, z_dim)\n",
    "fake_images = G2(z)\n",
    "p = D2(fake_images)\n",
    "d_loss2,dp = loss_func(p,fake_labels)\n",
    "dx = D2.backward(dp)\n",
    "G2.backward(dx)\n",
    "\n",
    "\n",
    "print(torch.FloatTensor(D2.modules[0]._dw))\n",
    "print(torch.FloatTensor(G2.modules[0]._dw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T17:09:53.348295Z",
     "start_time": "2020-08-11T17:09:53.303175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.8142e-03,  1.8691e-03, -3.9583e-04,  ..., -3.3681e-04,\n",
      "         -3.4774e-03,  9.9738e-04],\n",
      "        [ 3.7846e-03,  1.8627e-03, -3.2522e-04,  ..., -4.7253e-04,\n",
      "         -3.4235e-03,  1.0052e-03],\n",
      "        [ 3.8580e-03,  1.8610e-03, -5.8603e-04,  ..., -2.3947e-04,\n",
      "         -3.5073e-03,  1.0130e-03],\n",
      "        ...,\n",
      "        [ 3.6917e-03,  1.8559e-03, -2.7191e-04,  ..., -5.0256e-04,\n",
      "         -3.4215e-03,  1.0035e-03],\n",
      "        [ 4.3111e-03,  1.8918e-03, -1.1503e-03,  ...,  3.3949e-04,\n",
      "         -3.6607e-03,  1.0602e-03],\n",
      "        [ 4.0914e-03,  1.8685e-03, -8.6573e-04,  ...,  5.8522e-05,\n",
      "         -3.5890e-03,  1.0387e-03]])\n",
      "tensor([[ 2.4733e-05, -1.7784e-05,  3.8453e-05,  ..., -5.9857e-06,\n",
      "          1.1233e-04,  1.5950e-04],\n",
      "        [ 1.2189e-05,  5.1221e-05,  9.4727e-05,  ..., -1.0480e-04,\n",
      "         -5.1126e-06, -1.3556e-04],\n",
      "        [-2.9337e-05, -6.1800e-05, -5.6997e-05,  ..., -9.5516e-05,\n",
      "          1.3722e-05,  6.8221e-05],\n",
      "        ...,\n",
      "        [-2.8875e-05,  1.7340e-05,  7.1711e-05,  ..., -2.1557e-05,\n",
      "          2.5385e-05,  6.0189e-05],\n",
      "        [ 1.7014e-05,  3.4470e-05,  3.8829e-05,  ..., -7.7392e-05,\n",
      "          3.2902e-06,  3.6874e-05],\n",
      "        [-8.1480e-05,  4.3020e-06,  5.3887e-05,  ...,  7.5071e-05,\n",
      "          1.6376e-05,  2.3658e-05]])\n"
     ]
    }
   ],
   "source": [
    "p = D1(torch.FloatTensor(images))\n",
    "loss = F.binary_cross_entropy(p,torch.FloatTensor(real_labels))\n",
    "\n",
    "fake_images = G1(torch.FloatTensor(z))\n",
    "p = D1(torch.FloatTensor(fake_images))\n",
    "loss += F.binary_cross_entropy(p,torch.FloatTensor(fake_labels))\n",
    "\n",
    "\n",
    "loss.backward()\n",
    "print(D1[0].weight.grad.T)\n",
    "print(G1[0].weight.grad.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
