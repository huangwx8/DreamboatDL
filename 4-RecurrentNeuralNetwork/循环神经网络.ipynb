{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为什么需要RNN\n",
    "RNN是一种拥有记忆的网络, 一旦网络接收到了输入, 就会改变它的隐藏变量. 这个隐藏变量会参与RNN的前向运算, 从而让之前的输入x, 能影响现在的输出o. 具有这种性质的它通常用于处理序列信息. 序列信息比起之前的传统模式分类有着一些不太好的性质, 语音和文本信号, 都是变长的. 而且文本信息常常是每个词对应一个one-hot编码或者一个word-embedding词向量, 而语音信号的采样率又非常高, 一段简单的声音有可能对应着几千长度的序列. 变长的数据还比较容易处理, 毕竟我们在数据科学中也会遇到缺失值, 直接补0即可. 但是考虑到数据可能很长也可能很短, 我们想把它们一同处理就必须把所有数据对齐最长的那个.这样就造成了不必要的算力浪费. 最长的数据的维度可能很高(几k甚至几百k), 也就是至少我们输入层的input_size就很大, 即至少输入层的参数会非常非常多. 如果设计一个巨型的网络来处理序列数据显然是浪费的.  \n",
    "我们需要更好的架构来处理序列数据, 这时RNN就很有用了. RNN可以自由处理变长序列, 但是每次运算的输入只有一个词向量那么大. 这样就大大节约了参数. 同时也减少了计算.  \n",
    "### 结构与前向传播\n",
    "![rnn_arch.PNG](rnn_arch.PNG)\n",
    "RNN的计算和前馈网络相似, 每次RNN前向传播会同时接收两个向量, 一个是我们当前时刻的输入x, 另一个是保存在存储单元中的向量h. 我们会同时用这两个向量, 经过两个线性层, 得到RNN的隐层输出. 这个隐层输出会成为新的存储单元中的向量h, 参与下一次运算. 而当前时刻我们还会把这个h经过输出线性层, 再经过一个激活函数(softmax)得到当前时刻的输出.这个过程如果进行计算图展开就可以写成\n",
    "![rnn_unfold.png](rnn_unfold.png)\n",
    "我们的参数一共有三个线性层, 三个权重矩阵和三个偏置. 我们在实践时一般会把W和U对应的偏置合二为一, 也就是这样的RNN架构需要5种参数.  \n",
    "从0时刻开始, 我们的h一开始会被初始化为0. 然后, 我们用两个线性层和一个激活函数计算新的h.\n",
    "$$ h_1 = \\sigma(x_1U+h_0W+b) $$\n",
    "当前时刻的输出就由h1继续运算得到\n",
    "$$ o_1 = h_1V+c $$\n",
    "然后, 我们会接受新的x输入, 它和新的h一共继续这样运算下去\n",
    "$$ h_t = \\sigma(x_tU+h_{t-1}W+b) $$\n",
    "$$ o_t = h_tV+c $$\n",
    "这就是最简单的RNN架构, 如果让它接收完一整个序列信息, 他就可以输出一个和整个序列都有相关性的输出, 然后根据我们想要什么, 就可以设置合适的损失函数, 并用梯度方法训练它. \n",
    "### 反向传播(BPTT)\n",
    "RNN的参数梯度该如何计算呢? 如果你使用Pytorch的计算图模型来计算梯度, 就会发现这其实并不需要任何其他的backward_fn的设计, 因为我们只是用了一些激活函数和线性层, 我们之前的推导已经完全够用. 唯一需要注意的点是, 我们在计算图中进行了权值共享, 把同一个V,W,U使用了好多遍, 这时要计算导数时, 就需要把每个V,W,U的导数都计算一次, 然后把它们加起来.  \n",
    "![rnn_bp.PNG](rnn_bp.PNG)\n",
    "这里我们先计算出图中每个部分导, 然后再给出参数的导数到底该怎么计算的公式.  \n",
    "首先t时刻的输出损失$L_t$和t时刻的h是有直接相关性的, 因为$o_t = h_tV+c$, 我们这里可以直接计算V和c的偏导, 并计算出h关于$L_t$的偏导. 注意这并不是h的全部偏导, 我们还要考虑来自t+x时间的损失$L_{t+x}$的导数.设序列的总长度为K.\n",
    "$$ \\frac{\\partial L_t}{\\partial V} = h_t^T\\frac{\\partial L_t}{\\partial o_t} $$\n",
    "$$ \\frac{\\partial L_t}{\\partial c} = SUMROW\\ \\frac{\\partial L_t}{\\partial o_t} $$\n",
    "$$ \\frac{\\partial L_t}{\\partial h_t} = \\frac{\\partial L_t}{\\partial o_t}V^T $$\n",
    "$$ \\frac{\\partial L}{\\partial V} = \\sum_{k=1}^K h_k^T\\frac{\\partial L_k}{\\partial y_k} $$\n",
    "$$ \\frac{\\partial L}{\\partial c} = \\sum_{k=1}^K SUMROW\\ \\frac{\\partial L_k}{\\partial y_k} $$\n",
    "从上图我们知道, 任意$h_t$关于损失的导数要同时考虑$L_t$到$L_k$所有这些的损失. 对一个$L_k, k>t$, 我们要计算它对$h_t$的导数如下\n",
    "$$ \\frac{\\partial L_k}{\\partial h_t} = \\frac{\\partial L_k}{\\partial h_k}\\prod_{i=t}^{k-1}\\frac{\\partial h_{i+1}}{\\partial h_{i}} $$\n",
    "$$ \\frac{\\partial h_{i+1}}{\\partial h_{i}} = \\sigma'W^T $$\n",
    "如果我们使用tanh激活函数, 设$v_{t+1} = x_{t+1}U+h_tW+b$, $h_{t+1} = tanh(v_{t+1})$, 则能写出$\\frac{\\partial h_{i+1}}{\\partial h_{i}}$更精确的形式.\n",
    "$$ \\frac{\\partial h_{i+1}}{\\partial h_{i}} = (1-h_{i+1}^2)\\cdot W^T $$\n",
    "这样我们就能给出任意$h_t$关于总损失L的导数完整的形式\n",
    "$$ \\frac{\\partial L}{\\partial h_t} = \\sum_{k = t}^K\\frac{\\partial L_k}{\\partial h_t} = \\sum_{k = t}^K\\frac{\\partial L_k}{\\partial h_k}\\prod_{i=t}^{k-1}(1-h_{i+1}^2)\\cdot W^T $$\n",
    "然后任务就是根据$\\frac{\\partial L}{\\partial h_{t}}$计算W,U和b的导数. 虽然W,U,b是权值共享, 我们还是把不同时刻的它们写成$W_t,U_t,b_t$方便描述\n",
    "$$ \\frac{\\partial L}{\\partial W_t} = h_{t-1}^T\\sigma'\\frac{\\partial L}{\\partial h_{t}} = h_{t-1}^T ((1-h_{t}^2)\\cdot \\frac{\\partial L}{\\partial h_{t}}) $$\n",
    "$$ \\frac{\\partial L}{\\partial U_t} = x_{t}^T\\sigma'\\frac{\\partial L}{\\partial h_{t}} = x_{t}^T ((1-h_{t}^2)\\cdot \\frac{\\partial L}{\\partial h_{t}}) $$\n",
    "$$ \\frac{\\partial L}{\\partial b_t} = SUMROW\\ \\sigma'\\frac{\\partial L}{\\partial h_{t}} = SUMROW\\ ((1-h_{t}^2)\\cdot \\frac{\\partial L}{\\partial h_{t}}) $$\n",
    "权值共享的参数, 最后更新时要把这些不同时刻t得到的导数加起来, 才是最终的损失函数关于参数的导数\n",
    "$$ \\frac{\\partial L}{\\partial W} = \\sum_{k=1}^K \\frac{\\partial L}{\\partial W_k} $$\n",
    "$$ \\frac{\\partial L}{\\partial U} = \\sum_{k=1}^K \\frac{\\partial L}{\\partial U_k} $$\n",
    "$$ \\frac{\\partial L}{\\partial b} = \\sum_{k=1}^K \\frac{\\partial L}{\\partial b_k} $$\n",
    "### 训练技巧\n",
    "很显然, RNN无法像CNN一样, 实现超高度的并行化. CNN中, 我们可以在图像与图像间并行计算, 也能在卷积核与卷积核间并行计算. 但是RNN不行, RNN任意时刻的输入都取决于前面时刻的运算, 直到t-1时刻的运算完成, t时刻的运算才能开始. 这就使得RNN的运算很缓慢, 尽管如此, 我们还是希望尽可能并行化计算. 之前使用的mini-batch实际上仍然能在RNN中使用. 虽然一个batch中的数据有长有短, 我们只需要按照batch中最长的那个把所有数据在时间上对齐, 不足的补零. 这样时刻t就能一次输入n个m维的向量x, 即输入是n行m列矩阵$\\{x_{1t};x_{2t}...,x_{nt}\\}$.   \n",
    "另外RNN的训练有一些很不好的性质, 我们看$\\frac{\\partial L}{\\partial h_{t}}$的公式, 它是把很多的$\\frac{\\partial h_{t+1}}{\\partial h_{t}}=\\frac{1}{1-v_{i+1}^2}\\cdot W^T$做连乘, 而因为$W$权值共享, 每次的$\\frac{\\partial h_{t+1}}{\\partial h_{t}}$相差并不会很大. 设想, 如果所有$\\frac{\\partial h_{t+1}}{\\partial h_{t}}\\simeq 1.01$, 序列长度为1000, 对$\\frac{\\partial L}{\\partial h_{1}}$计算导数将会有1.01^{100} = 20959那么大, 这就是RNN的短板, 梯度爆炸. 在优化的目标函数中会非常常见这样的\"悬崖\", 如果我们用这个梯度更新参数, 将会让参数直接废掉. 为此我们会用一些简单的处理方式来缓解这种影响, 比如每隔T个时间单位就清零前面传来的梯度, 或是更新参数时设计clip截断梯度.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T05:34:01.773715Z",
     "start_time": "2020-08-11T05:34:00.790373Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "path = os.getcwd()\n",
    "os.chdir('..')\n",
    "from deepnotes import *\n",
    "os.chdir(path)\n",
    "from sklearn.metrics import accuracy_score\n",
    "# 用于验算梯度\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不含输出层，仅含输入层和隐层两个线性层的RNN只需要不断计算下面的式子即可\n",
    "$$ h_t = \\sigma(x_tU+h_{t-1}W+b) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T05:34:02.697702Z",
     "start_time": "2020-08-11T05:34:01.776710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9418, -0.5154],\n",
      "        [-0.9824,  0.9829],\n",
      "        [-0.5701,  0.9826],\n",
      "        [ 0.7563,  0.6120]], grad_fn=<TanhBackward>)\n",
      "dw_ih:\n",
      " tensor([[-0.0239, -1.8457],\n",
      "        [-1.4684,  1.5550],\n",
      "        [ 2.3151,  1.2533]])\n",
      "dw_hh:\n",
      " tensor([[-1.1513, -1.1194],\n",
      "        [ 0.6028,  2.6666]])\n",
      "db:\n",
      " tensor([ 4.4169, -0.6943])\n"
     ]
    }
   ],
   "source": [
    "weight_ih = torch.randn(3,2)\n",
    "weight_hh = torch.randn(2,2)\n",
    "bias = torch.zeros(2)\n",
    "\n",
    "weight_ih.requires_grad_(True)\n",
    "weight_hh.requires_grad_(True)\n",
    "bias.requires_grad_(True)\n",
    "\n",
    "seq_len = 4\n",
    "\n",
    "h = torch.zeros((4,2))\n",
    "x = torch.randn(4,seq_len,3)\n",
    "y = torch.randn(4,2)\n",
    "\n",
    "for t in range(seq_len):\n",
    "    v = x[:,t,:].mm(weight_ih)+h.mm(weight_hh)+bias\n",
    "    h = torch.tanh(v)\n",
    "\n",
    "print(h)\n",
    "    \n",
    "loss = F.mse_loss(h,y,reduction='sum')\n",
    "loss.backward()\n",
    "\n",
    "print('dw_ih:\\n',weight_ih.grad)\n",
    "print('dw_hh:\\n',weight_hh.grad)\n",
    "print('db:\\n',bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T05:34:02.716752Z",
     "start_time": "2020-08-11T05:34:02.699708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.94184225 -0.51541592]\n",
      " [-0.98241899  0.98293309]\n",
      " [-0.57011113  0.98261379]\n",
      " [ 0.7562784   0.61203227]]\n",
      "dw_ih:\n",
      " [[-0.02388615 -1.84567514]\n",
      " [-1.46839742  1.55497676]\n",
      " [ 2.3151302   1.25326869]]\n",
      "dw_hh:\n",
      " [[-1.1513377  -1.11943905]\n",
      " [ 0.60276456  2.66664582]]\n",
      "db:\n",
      " [ 4.41694133 -0.69426939]\n"
     ]
    }
   ],
   "source": [
    "x = x.numpy()\n",
    "y = y.numpy()\n",
    "\n",
    "rnn = RNN(3,2)\n",
    "rnn.weight_ih = weight_ih.detach().numpy()\n",
    "rnn.weight_hh = weight_hh.detach().numpy()\n",
    "rnn.bias = bias.detach().numpy()\n",
    "\n",
    "h = rnn(x)\n",
    "rnn.backward(2*(h-y))\n",
    "\n",
    "print(h)\n",
    "\n",
    "print('dw_ih:\\n',rnn._dw_ih)\n",
    "print('dw_hh:\\n',rnn._dw_hh)\n",
    "print('db:\\n',rnn._db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 姓名国别分类\n",
    "不同国家的姓名的字母组成有着独特的特色，使用RNN可以高效地识别这种组合特征，并完成分类任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T05:34:02.811010Z",
     "start_time": "2020-08-11T05:34:02.719761Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_letters: 26\n",
      "n_categories: 18\n",
      "category_lines:['abandonato', 'abatangelo', 'abatantuono', 'abate', 'abategiovanni']\n"
     ]
    }
   ],
   "source": [
    "from io import open\n",
    "import glob\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "\n",
    "def findFiles(path):\n",
    "    return glob.glob(path)\n",
    "\n",
    "# 只保留小写字母\n",
    "all_letters = string.ascii_lowercase\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    lines = [''.join(list(filter(str.isalpha, line))).lower() for line in lines]\n",
    "    return lines\n",
    "\n",
    "for filename in findFiles('names/*.txt'):\n",
    "    category = filename.split('/')[-1].split('.')[0]\n",
    "    category = category.split('\\\\')[1]\n",
    "    # 18种类别\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    # category_lines：字典类型（类别: 名字list）\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "\n",
    "print(\"all_letters: \"+str(len(all_letters)))\n",
    "print(\"n_categories: \"+str(n_categories))\n",
    "print(\"category_lines:\"+str(category_lines['Italian'][:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T05:34:02.832071Z",
     "start_time": "2020-08-11T05:34:02.815022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 使用one hot encoding\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "# 字母所在位置\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToMatrix(line, seq_len):\n",
    "    oh_mat = np.zeros((seq_len, n_letters))\n",
    "    indices = [all_letters.find(letter) for letter in line]\n",
    "    indices += [0]*(seq_len-len(indices))\n",
    "    oh_mat[range(seq_len),indices] = 1\n",
    "    return oh_mat\n",
    "\n",
    "print(lineToMatrix('Jones',5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T05:34:02.858135Z",
     "start_time": "2020-08-11T05:34:02.835074Z"
    }
   },
   "outputs": [],
   "source": [
    "# 随机采样训练样本对\n",
    "import random\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample(n_samples):\n",
    "    x_samples = []\n",
    "    y_samples = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        category = randomChoice(all_categories)\n",
    "        line = randomChoice(category_lines[category])\n",
    "        x_samples.append(line)\n",
    "        y_samples.append(all_categories.index(category))\n",
    "    \n",
    "    seq_len = max(len(l) for l in x_samples)\n",
    "    \n",
    "    line_mat = [lineToMatrix(l,seq_len) for l in x_samples]\n",
    "    line_mat = np.stack(line_mat)\n",
    "    return line_mat,np.array(y_samples),line,category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T05:39:17.988395Z",
     "start_time": "2020-08-11T05:34:02.863149Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [1000/100000], Loss: 3.3742\n",
      "gagnon / English ✗ (French)\n",
      "Step [2000/100000], Loss: 2.8794\n",
      "peck / Greek ✗ (English)\n",
      "Step [3000/100000], Loss: 2.6554\n",
      "walker / Scottish ✓\n",
      "Step [4000/100000], Loss: 2.4007\n",
      "pan / Chinese ✗ (French)\n",
      "Step [5000/100000], Loss: 2.2640\n",
      "safar / Arabic ✓\n",
      "Step [6000/100000], Loss: 2.1901\n",
      "hynna / Vietnamese ✗ (Czech)\n",
      "Step [7000/100000], Loss: 2.1383\n",
      "vespa / Spanish ✗ (Italian)\n",
      "Step [8000/100000], Loss: 1.9957\n",
      "richards / French ✗ (English)\n",
      "Step [9000/100000], Loss: 1.8898\n",
      "yoshizawa / Japanese ✓\n",
      "Step [10000/100000], Loss: 1.7830\n",
      "rome / Spanish ✗ (French)\n",
      "Step [11000/100000], Loss: 1.7851\n",
      "rigatos / Greek ✓\n",
      "Step [12000/100000], Loss: 1.6827\n",
      "kalinochkin / Greek ✗ (Russian)\n",
      "Step [13000/100000], Loss: 1.6173\n",
      "atterton / Dutch ✗ (English)\n",
      "Step [14000/100000], Loss: 1.6051\n",
      "buggenum / Dutch ✓\n",
      "Step [15000/100000], Loss: 1.6087\n",
      "reinder / Dutch ✓\n",
      "Step [16000/100000], Loss: 1.4786\n",
      "nasato / Arabic ✗ (Italian)\n",
      "Step [17000/100000], Loss: 1.5427\n",
      "eltis / English ✓\n",
      "Step [18000/100000], Loss: 1.4673\n",
      "le / English ✗ (Vietnamese)\n",
      "Step [19000/100000], Loss: 1.4120\n",
      "welton / German ✗ (English)\n",
      "Step [20000/100000], Loss: 1.3264\n",
      "van / Vietnamese ✓\n",
      "Step [21000/100000], Loss: 1.3906\n",
      "shon / Korean ✓\n",
      "Step [22000/100000], Loss: 1.3585\n",
      "haanraats / Dutch ✓\n",
      "Step [23000/100000], Loss: 1.3011\n",
      "tihotsky / Russian ✓\n",
      "Step [24000/100000], Loss: 1.3032\n",
      "kieu / Scottish ✗ (Vietnamese)\n",
      "Step [25000/100000], Loss: 1.2946\n",
      "donk / Chinese ✗ (Dutch)\n",
      "Step [26000/100000], Loss: 1.2406\n",
      "labelle / French ✓\n",
      "Step [27000/100000], Loss: 1.2300\n",
      "forakis / Greek ✓\n",
      "Step [28000/100000], Loss: 1.2296\n",
      "rovigatti / Italian ✓\n",
      "Step [29000/100000], Loss: 1.2573\n",
      "paternoster / German ✗ (French)\n",
      "Step [30000/100000], Loss: 1.2318\n",
      "houlis / Greek ✓\n",
      "Step [31000/100000], Loss: 1.1696\n",
      "henderson / Scottish ✓\n",
      "Step [32000/100000], Loss: 1.1004\n",
      "saito / Italian ✗ (Japanese)\n",
      "Step [33000/100000], Loss: 1.1695\n",
      "gorecki / Polish ✓\n",
      "Step [34000/100000], Loss: 1.1099\n",
      "ventura / Portuguese ✓\n",
      "Step [35000/100000], Loss: 1.1180\n",
      "koumans / Greek ✗ (Dutch)\n",
      "Step [36000/100000], Loss: 1.0808\n",
      "sutherland / Scottish ✓\n",
      "Step [37000/100000], Loss: 1.0652\n",
      "maclean / Scottish ✓\n",
      "Step [38000/100000], Loss: 1.0619\n",
      "araújo / Portuguese ✓\n",
      "Step [39000/100000], Loss: 1.0788\n",
      "yuhnev / Russian ✓\n",
      "Step [40000/100000], Loss: 1.0311\n",
      "awad / Arabic ✓\n",
      "Step [41000/100000], Loss: 0.9745\n",
      "kwang / Korean ✓\n",
      "Step [42000/100000], Loss: 0.9679\n",
      "smeets / Dutch ✓\n",
      "Step [43000/100000], Loss: 0.9777\n",
      "wilson / Scottish ✓\n",
      "Step [44000/100000], Loss: 0.9582\n",
      "thuy / Vietnamese ✓\n",
      "Step [45000/100000], Loss: 0.9414\n",
      "franco / Portuguese ✓\n",
      "Step [46000/100000], Loss: 0.9718\n",
      "burns / German ✗ (Scottish)\n",
      "Step [47000/100000], Loss: 0.9878\n",
      "kikui / Japanese ✓\n",
      "Step [48000/100000], Loss: 0.9405\n",
      "coelho / Portuguese ✓\n",
      "Step [49000/100000], Loss: 0.9193\n",
      "bonhomme / French ✓\n",
      "Step [50000/100000], Loss: 0.9026\n",
      "mihashi / Japanese ✓\n",
      "Step [51000/100000], Loss: 0.9283\n",
      "vykhodtsev / Russian ✓\n",
      "Step [52000/100000], Loss: 0.8837\n",
      "balabuha / Russian ✓\n",
      "Step [53000/100000], Loss: 0.8649\n",
      "kwang / Korean ✓\n",
      "Step [54000/100000], Loss: 0.8156\n",
      "shon / Korean ✓\n",
      "Step [55000/100000], Loss: 0.8144\n",
      "salomon / French ✗ (Italian)\n",
      "Step [56000/100000], Loss: 0.8448\n",
      "czajka / Polish ✓\n",
      "Step [57000/100000], Loss: 0.8385\n",
      "tsahilov / Russian ✓\n",
      "Step [58000/100000], Loss: 0.8377\n",
      "gallchobhar / Irish ✓\n",
      "Step [59000/100000], Loss: 0.8235\n",
      "gravari / Italian ✗ (Greek)\n",
      "Step [60000/100000], Loss: 0.8097\n",
      "abascal / Spanish ✓\n",
      "Step [61000/100000], Loss: 0.7963\n",
      "mar / Chinese ✓\n",
      "Step [62000/100000], Loss: 0.7957\n",
      "callaghan / Irish ✓\n",
      "Step [63000/100000], Loss: 0.7828\n",
      "estéves / Portuguese ✓\n",
      "Step [64000/100000], Loss: 0.7544\n",
      "filipowski / Polish ✓\n",
      "Step [65000/100000], Loss: 0.7879\n",
      "kim / Korean ✓\n",
      "Step [66000/100000], Loss: 0.7269\n",
      "sokolowski / Polish ✓\n",
      "Step [67000/100000], Loss: 0.8325\n",
      "kloeten / Dutch ✓\n",
      "Step [68000/100000], Loss: 0.8378\n",
      "albuquerque / Spanish ✗ (Portuguese)\n",
      "Step [69000/100000], Loss: 0.6794\n",
      "horowitz / German ✓\n",
      "Step [70000/100000], Loss: 0.7418\n",
      "svocak / Czech ✓\n",
      "Step [71000/100000], Loss: 0.7256\n",
      "middelburg / Dutch ✓\n",
      "Step [72000/100000], Loss: 0.6787\n",
      "juravkov / Russian ✓\n",
      "Step [73000/100000], Loss: 0.7321\n",
      "clark / Scottish ✗ (Irish)\n",
      "Step [74000/100000], Loss: 0.7364\n",
      "plisek / Czech ✓\n",
      "Step [75000/100000], Loss: 0.6942\n",
      "lunt / Irish ✗ (English)\n",
      "Step [76000/100000], Loss: 0.6712\n",
      "alberghini / Italian ✓\n",
      "Step [77000/100000], Loss: 0.6315\n",
      "adleroff / English ✗ (Russian)\n",
      "Step [78000/100000], Loss: 0.7006\n",
      "estéves / Portuguese ✓\n",
      "Step [79000/100000], Loss: 0.6941\n",
      "stepan / Czech ✓\n",
      "Step [80000/100000], Loss: 0.6417\n",
      "faolan / Irish ✓\n",
      "Step [81000/100000], Loss: 0.6719\n",
      "abalmasoff / Russian ✓\n",
      "Step [82000/100000], Loss: 0.6990\n",
      "handokhin / Russian ✓\n",
      "Step [83000/100000], Loss: 0.7029\n",
      "eoin / Irish ✓\n",
      "Step [84000/100000], Loss: 0.6292\n",
      "avalishvili / Japanese ✗ (Russian)\n",
      "Step [85000/100000], Loss: 0.6413\n",
      "miyake / Japanese ✓\n",
      "Step [86000/100000], Loss: 0.6537\n",
      "yeon / Korean ✓\n",
      "Step [87000/100000], Loss: 0.6200\n",
      "aodha / Japanese ✗ (Irish)\n",
      "Step [88000/100000], Loss: 0.6534\n",
      "paternoster / French ✓\n",
      "Step [89000/100000], Loss: 0.6213\n",
      "quach / Vietnamese ✓\n",
      "Step [90000/100000], Loss: 0.6159\n",
      "fish / Irish ✗ (English)\n",
      "Step [91000/100000], Loss: 0.6413\n",
      "mo / Korean ✓\n",
      "Step [92000/100000], Loss: 0.6422\n",
      "sargent / French ✗ (English)\n",
      "Step [93000/100000], Loss: 0.6068\n",
      "creasey / English ✓\n",
      "Step [94000/100000], Loss: 0.6184\n",
      "souza / Portuguese ✓\n",
      "Step [95000/100000], Loss: 0.6009\n",
      "cavey / French ✓\n",
      "Step [96000/100000], Loss: 0.5994\n",
      "hemmings / English ✓\n",
      "Step [97000/100000], Loss: 0.5765\n",
      "bursinos / Greek ✓\n",
      "Step [98000/100000], Loss: 0.5924\n",
      "naifeh / Arabic ✓\n",
      "Step [99000/100000], Loss: 0.6374\n",
      "pantelas / Greek ✓\n",
      "Step [100000/100000], Loss: 0.6381\n",
      "noel / French ✓\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Train loss(Cross Entropy)')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x238f2dbc5f8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential(\n",
    "    RNN(n_letters,180,requires_clip=True),\n",
    "    Linear(180,n_categories),\n",
    ")\n",
    "loss_func = CrossEntropyLossWithSoftMax(n_categories)\n",
    "optimizer = SGD(0.001)\n",
    "model.apply_optim(optimizer)\n",
    "loss_list = []\n",
    "running_loss = 0.\n",
    "num_iters = 100000\n",
    "log_step = 1000\n",
    "\n",
    "for i in range(num_iters):\n",
    "    x,y,name,category = randomTrainingExample(1)\n",
    "    # Forward pass\n",
    "    logits = model(x)\n",
    "    # calculate loss\n",
    "    loss,dlogits = loss_func(logits,y)\n",
    "    # Backward\n",
    "    model.zero_grad()\n",
    "    model.backward(dlogits)\n",
    "    # optimize\n",
    "    optimizer.step()\n",
    "    \n",
    "    running_loss += loss.item()\n",
    "\n",
    "    if (i+1) % log_step == 0:\n",
    "        running_loss /= log_step\n",
    "        print ('Step [{}/{}], Loss: {:.4f}'.format(i+1, num_iters, running_loss))\n",
    "        loss_list.append(running_loss)\n",
    "        guess = all_categories[np.argmax(logits,axis = 1)[0]]\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%s / %s %s'%(name,guess,correct))\n",
    "        running_loss = 0.\n",
    "\n",
    "plt.plot(loss_list)\n",
    "plt.title('Train loss(Cross Entropy)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T05:39:18.015466Z",
     "start_time": "2020-08-11T05:39:17.991904Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">hinton\n",
      "top 1: English (5.2533)\n",
      "top 2: Scottish (3.9399)\n",
      "top 3: German (2.8684)\n",
      ">lecun\n",
      "top 1: English (4.7236)\n",
      "top 2: French (4.5072)\n",
      "top 3: German (4.2887)\n",
      ">goodfellow\n",
      "top 1: Russian (4.4157)\n",
      "top 2: English (4.0306)\n",
      "top 3: Greek (3.5681)\n",
      ">ming\n",
      "top 1: Chinese (7.2009)\n",
      "top 2: Korean (4.6924)\n",
      "top 3: Vietnamese (4.4949)\n",
      ">satoshi\n",
      "top 1: Italian (4.8519)\n",
      "top 2: Polish (4.0977)\n",
      "top 3: Japanese (4.0213)\n"
     ]
    }
   ],
   "source": [
    "names = ['hinton','lecun','goodfellow','ming','satoshi']\n",
    "\n",
    "for name in names:\n",
    "    mat = lineToMatrix(name,len(name))\n",
    "    x = mat[np.newaxis,:]\n",
    "    y_pred = model(x)[0]\n",
    "    indices = np.argsort(-y_pred)[:3]\n",
    "    print('>'+name)\n",
    "    for i,idx in enumerate(indices):\n",
    "        print('top %d:'%(i+1),all_categories[idx],'(%.4f)'%(y_pred[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "![lstm.jpg](lstm.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
